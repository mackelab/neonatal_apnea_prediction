{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from src.misc_utils import binary_classifier_performance, read_pickle_file\n",
    "from src.plotting_utils import plot_auroc_with_uncertainty\n",
    "\n",
    "matplotlibrc_path = \"../matplotlibrc\"\n",
    "result_path = \"TODO\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = \"TODO\"\n",
    "multi_run = f\"{exp_id}_main_multirun\"\n",
    "num_run = \"01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = read_pickle_file(\n",
    "    f\"{exp_id}_run_{num_run}_config.pkl\", os.path.join(result_path, multi_run)\n",
    ")\n",
    "models_dict = read_pickle_file(\n",
    "    f\"{exp_id}_run_{num_run}_models.pkl\", os.path.join(result_path, multi_run)\n",
    ")\n",
    "results_dict = read_pickle_file(\n",
    "    f\"{exp_id}_run_{num_run}_results.pkl\", os.path.join(result_path, multi_run)\n",
    ")\n",
    "\n",
    "all_results = []\n",
    "lag_exp = f\"{exp_id}_lag_experiment\"\n",
    "for mod in [15 * i for i in range(11)]:\n",
    "    all_results.append(\n",
    "        read_pickle_file(\n",
    "            f\"{exp_id}_lag{mod}_results.pkl\", os.path.join(result_path, lag_exp)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ls, labels_ls, individual_aucs = [], [], []\n",
    "for key in results_dict:\n",
    "    tup = results_dict[key]\n",
    "    labels = tup[\"ys\"]\n",
    "    scores = tup[\"scores\"]\n",
    "    scores_ls.append(scores)\n",
    "    labels_ls.append(labels)\n",
    "    individual_aucs.append(roc_auc_score(labels, scores))\n",
    "\n",
    "with plt.rc_context(fname=matplotlibrc_path):\n",
    "    overall_avg_auc, overall_std_auc = plot_auroc_with_uncertainty(scores_ls, labels_ls)\n",
    "\n",
    "    plt.plot(\n",
    "        np.linspace(0.0, 1.0, 50),\n",
    "        np.linspace(0.0, 1.0, 50),\n",
    "        color=\"grey\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"lower right\", fontsize=15)\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = 19\n",
    "num_lag = 11\n",
    "lag_step = 15\n",
    "\n",
    "auc = np.zeros((num_lag, num_subjects))\n",
    "acc = np.zeros((num_lag, num_subjects))\n",
    "\n",
    "for idx, res_dict in enumerate(all_results):\n",
    "    frac_pos_ls, roc_auc_ls, avg_prc_ls, f1_score_ls, acc_ls = [], [], [], [], []\n",
    "    for key in res_dict:\n",
    "        id_res_dic = res_dict[key]\n",
    "        labels = id_res_dic[\"ys\"]\n",
    "        preds = id_res_dic[\"scores\"]\n",
    "        frac_pos, roc_auc, avg_prc, f1_score, acci = binary_classifier_performance(\n",
    "            preds, labels, print_and_plot=False\n",
    "        )\n",
    "        frac_pos_ls.append(frac_pos)\n",
    "        roc_auc_ls.append(roc_auc)\n",
    "        f1_score_ls.append(f1_score)\n",
    "        acc_ls.append(acci)\n",
    "    auc[idx] = np.array(roc_auc_ls)\n",
    "    acc[idx] = np.array(acc_ls)\n",
    "\n",
    "x = [lag_step * i for i in range(num_lag)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context(fname=matplotlibrc_path):\n",
    "    large_fontsize = 15\n",
    "    mid_fontsize = 10\n",
    "    small_fontsize = 8\n",
    "\n",
    "    figure_width = 12\n",
    "    with_to_height_ratio = 5 / 17\n",
    "\n",
    "    fig = plt.figure(figsize=(figure_width, with_to_height_ratio * figure_width))\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    fig.subplots_adjust(wspace=1.5, hspace=0.5)\n",
    "\n",
    "    ax = plt.subplot2grid((1, 12), loc=(0, 4), colspan=4, rowspan=1)\n",
    "    ax.set_title(r\"$\\bf{e}$\", loc=\"left\", fontsize=large_fontsize, y=1.05, x=-0.15)\n",
    "    ax.set_title(\"Individual performance\", fontsize=mid_fontsize)\n",
    "    width = 0.7\n",
    "    y_pos = np.arange(num_subjects)\n",
    "    individual_aucs = np.array(individual_aucs)\n",
    "    args = np.argsort(individual_aucs)\n",
    "    ax.bar(\n",
    "        y_pos,\n",
    "        individual_aucs[args],\n",
    "        width=width,\n",
    "        align=\"center\",\n",
    "        color=\"C2\",\n",
    "        label=\"AuROC\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        y_pos,\n",
    "        [0.5] * len(individual_aucs),\n",
    "        width=width,\n",
    "        align=\"center\",\n",
    "        alpha=0.5,\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(y_pos)\n",
    "    ax.set_xticklabels([f\"{i+1:02d}\" for i in args])\n",
    "    ax.set_ylim((0.0, 1.0))\n",
    "    ax.set_yticks([0.0, 0.5, 1.0])\n",
    "    ax.set_ylabel(\"AuROC\", fontsize=mid_fontsize)\n",
    "    ax.set_xlabel(\"Patient number\", fontsize=mid_fontsize)\n",
    "\n",
    "    ax = plt.subplot2grid((1, 12), loc=(0, 0), colspan=4, rowspan=1)\n",
    "    ax.set_title(r\"$\\bf{d}$\", loc=\"left\", fontsize=large_fontsize, y=1.05, x=-0.15)\n",
    "    ax.set_title(f\"Average AuROC: {overall_avg_auc}\", fontsize=mid_fontsize)\n",
    "    plot_auroc_with_uncertainty(\n",
    "        scores_ls, labels_ls, color=\"C2\", label=\"AuROC: \", alpha_uncertainty=0.3\n",
    "    )\n",
    "    ax.plot(\n",
    "        np.linspace(0.0, 1.0, 50),\n",
    "        np.linspace(0.0, 1.0, 50),\n",
    "        color=\"grey\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    ax.set_xlim((0.0, 1.0))\n",
    "    ax.set_ylim((0.0, 1.0))\n",
    "    ax.set_xticks([0.0, 0.5, 1.0])\n",
    "    ax.set_yticks([0.0, 0.5, 1.0])\n",
    "\n",
    "    ax.set_xlabel(\"False positive rate\", fontsize=mid_fontsize)\n",
    "    ax.set_ylabel(\"True positive rate\", fontsize=mid_fontsize)\n",
    "\n",
    "    ax = plt.subplot2grid((1, 12), loc=(0, 8), colspan=4, rowspan=1)\n",
    "    ax.set_title(r\"$\\bf{f}$\", loc=\"left\", fontsize=large_fontsize, y=1.05, x=-0.15)\n",
    "    ax.set_title(\n",
    "        \"Performance for increasing\\nprediction horizon\", fontsize=mid_fontsize, y=0.95\n",
    "    )\n",
    "    ax.plot(x, np.mean(auc, axis=1), \"-s\", color=\"C2\")\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        np.mean(auc, axis=1) - np.std(auc, axis=1),\n",
    "        np.mean(auc, axis=1) + np.std(auc, axis=1),\n",
    "        alpha=0.3,\n",
    "        color=\"C2\",\n",
    "    )\n",
    "    ax.set_ylabel(\"AuROC\", fontsize=mid_fontsize)\n",
    "    ax.set_xlabel(\"prediction horizon (seconds)\", fontsize=mid_fontsize)\n",
    "    ax.set_ylim((0.4, 1.0))\n",
    "    ax.set_yticks([0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
